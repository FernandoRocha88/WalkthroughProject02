{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a cluster model to group australian cities/states based on weather information\n",
        "* Understand profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster model\n",
        "* Classifier modeel to explain clusters\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "\n",
        "* how to translate cluster to map?\n",
        "  * dataset is time series, each row is a day for each city\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "! pip install yellowbrick==1.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "# import uuid\n",
        "# file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "# with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "# print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "# print(\"\\n\\n\")\n",
        "# os.remove(f\"{file_name}.txt\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\")\n",
        "      .drop(['RainTomorrow','RainfallTomorrow','RainYesterday'],axis=1) # drop target variables\n",
        ")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kD7PZ5kZkBT"
      },
      "source": [
        "## Custom transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96j6zLKZz7N"
      },
      "source": [
        "  * convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "  * get Get Day, Month, Year, Weekday, IsWeekend from Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTrXFaWSIU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "class ConvertToCategorical(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variables=None):\n",
        "      if not isinstance(variables, list):\n",
        "          self.variables = [variables]\n",
        "      else:\n",
        "          self.variables = variables\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      for feature in self.variables:\n",
        "          X[feature] = X[feature].astype('object')\n",
        "\n",
        "      return X\n",
        "\n",
        "\n",
        "# Get Day, Month, Year, Weekday, IsWeekend from Date\n",
        "class GetFeaturesFromDate(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variable=None):\n",
        "      self.variable = variable\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      X[self.variable] = pd.to_datetime(X[self.variable])\n",
        "      X['Day'] = X[self.variable].dt.day\n",
        "      X['Month'] = X[self.variable].dt.month\n",
        "      X['Year'] = X[self.variable].dt.year\n",
        "      X['WeekDay']= X[self.variable].dt.weekday\n",
        "      X['IsWeekend'] = X['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "      return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML Pipeline: Base, Cluster and ClfToExplainClusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.imputation import AddMissingIndicator\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import DropMissingData\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "from feature_engine.encoding import RareLabelEncoder,OrdinalEncoder\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      ### Data Cleaning\n",
        "      (\"ConvertToCategorical\",ConvertToCategorical(variables = ['Cloud9am','Cloud3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"GetFeaturesFromDate\",GetFeaturesFromDate(variable= 'Date')\n",
        "      ),\n",
        "       \n",
        "\n",
        "      (\"DropFeatures\",DropFeatures(features_to_drop = ['Evaporation','Cloud9am','Date','Sunshine'])\n",
        "      ),\n",
        "\n",
        "      (\"DropMissingData\",DropMissingData(variables =['RainfallToday', 'RainToday',\n",
        "                                                    #  'RainYesterday'\n",
        "                                                     ])\n",
        "      ),\n",
        "       \n",
        "\n",
        "\n",
        "      (\"CategoricalImputer\",CategoricalImputer(variables=['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm'],\n",
        "                                                imputation_method='missing',fill_value='Missing')\n",
        "      ),\n",
        "\n",
        "      (\"MedianImputer\",MeanMedianImputer(imputation_method='median',\n",
        "                                          variables=['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                                                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                                                    'WindSpeed9am','Temp9am','MaxTemp']\n",
        "                                          )\n",
        "      ),\n",
        "\n",
        "      (\"MeanImputer\",MeanMedianImputer(imputation_method='mean',variables=['MinTemp'])\n",
        "      ),\n",
        "\n",
        "      ### Feature Engineering\n",
        "\n",
        "      (\"RareLabelEncoder_tol5\",RareLabelEncoder(tol=0.05, n_categories=2, variables=['WindDir3pm'])\n",
        "      ),\n",
        "       \n",
        "      (\"RareLabelEncoder_tol7\",RareLabelEncoder(tol=0.06, n_categories=2, variables=['State'])\n",
        "      ),\n",
        "       \n",
        "\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary', \n",
        "                                                  variables = ['Location','WindGustDir','WindDir9am',\n",
        "                                                               'WindDir3pm','State','Cloud3pm',\n",
        "                                                               'RainToday',\n",
        "                                                              #  'RainYesterday'\n",
        "                                                               ])\n",
        "      ),\n",
        "\n",
        "\n",
        "      (\"Winsorizer_iqr\",Winsorizer(capping_method='iqr',tail='both', fold=3,variables = ['RainfallToday'])\n",
        "      ),\n",
        "\n",
        "\n",
        "      (\"PowerTransformer\",PowerTransformer(variables = ['WindSpeed3pm','Humidity3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"YeoJohnsonTransformer\",YeoJohnsonTransformer(variables=['WindGustSpeed','WindSpeed9am','Humidity9am',\n",
        "                                                                'RainfallToday'\n",
        "                                                                ])\n",
        "      ),\n",
        "\n",
        "      (\"EqualFrequencyDiscretiser\",EqualFrequencyDiscretiser(q=5,variables = ['Latitude','Longitude'])\n",
        "      ),\n",
        "\n",
        "\n",
        "       ####\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    ]\n",
        "  )\n",
        "  return pipeline_base\n",
        "\n",
        "\n",
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "  pipe.steps.append([\n",
        "                     \"PCA\",PCA(n_components=4, random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"model\",KMeans(n_clusters=4, random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  return pipe\n",
        "\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "   pipe.steps.append([\n",
        "                     \"feat_selection\",SelectFromModel(DecisionTreeClassifier(random_state=config.RANDOM_STATE))\n",
        "                     ])\n",
        "   \n",
        "   pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "   \n",
        "   pipe.steps.append([\n",
        "                     \"model\",DecisionTreeClassifier(random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "   return pipe\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "# Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fnArcWET0D"
      },
      "source": [
        "* It needs the dataset after data cleaning and feature engineering\n",
        "  * That means you have to remove 3 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-3])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "print(df_pca.shape)\n",
        "df_pca.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "* Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "n_components = 4\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFC6gDrfq83G"
      },
      "source": [
        "* Heatmap: PCA components and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plQLu3Z3tE5Y"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df_comp = pd.DataFrame(pca.components_, columns=df_pca.columns)\n",
        "plt.figure(figsize=(20,5))\n",
        "sns.heatmap(df_comp,center=0,linewidths=.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "# Elbow Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXb_H6iGx6U"
      },
      "source": [
        "* Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df)\n",
        "\n",
        "print(df_elbow.shape,'\\n')\n",
        "df_elbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KLQujEvgi"
      },
      "source": [
        "* Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(), k=(1,16))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "\n",
        "# 5 clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "* Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "X = df.copy()\n",
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU1E4IdkyLd"
      },
      "source": [
        "* Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lx5EqalOtv"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk0uyz1lSIW"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJzUxAmkv07"
      },
      "source": [
        "* The goal is to merge cluster labels to our data.\n",
        "  * However, the pipeline dropped rows from ['RainfallToday', 'RainToday']\n",
        "  * Before merging, we need to adjust it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUY6-_6A-rN"
      },
      "source": [
        "drop_imputer = DropMissingData(variables =['RainfallToday' , 'RainToday'])  #,'RainfallTomorrow','RainTomorrow'])\n",
        "X = drop_imputer.fit_transform(X)\n",
        "\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "* We add a column \"Cluster\" to the data and check clusters distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "X['Clusters'] = X['Clusters'].astype('object')\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OhXSh7536Ye"
      },
      "source": [
        "* Clusters don't look to be imbalanced\n",
        "* This is how our data look like from now\n",
        "  * Check the last column: Clusters\n",
        "  * Quick reminder: The data is unprocessed (data cleaning, feat eng); except for the part DropMissingData(variables =['RainfallToday', 'RainToday'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "source": [
        "print(X.shape)\n",
        "X.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Clusters Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4j4h-2Shlq4"
      },
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df)\n",
        "df_transformed.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqxawC8lq1R"
      },
      "source": [
        "from config import config\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "\n",
        "def EvaluateClusterSilhouette(X,Clusters):\n",
        "\n",
        "\tn_clusters = len(set(Clusters))\n",
        "\n",
        "\tprint(\" Silhouette plot for each cluster\")\n",
        "\tfig, (ax1) = plt.subplots(1, 1)\n",
        "\tfig.set_size_inches(18, 7)\n",
        "\tax1.set_xlim([-0.1, 1])\n",
        "\tax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\t\n",
        "\tsilhouette_avg = silhouette_score(X, cluster_labels,random_state=config.RANDOM_STATE)\n",
        "\tprint(\"* The silhouette average score is \",str(round(float(silhouette_avg),2)))\n",
        "\t# print(\n",
        "\t# \tf\"* Silhouette assesses consistency within clusters - \"\n",
        "\t# \tf\"[Link 1] (https://en.wikipedia.org/wiki/Silhouette_(clustering)) and \"\n",
        "\t# \tf\"[Link 2] (https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam) \")\n",
        "\n",
        "\n",
        "\tsample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\ty_lower = 10\n",
        "\tfor i in range(n_clusters):\n",
        "\t\tith_cluster_silhouette_values = \\\n",
        "\t\t\tsample_silhouette_values[cluster_labels == i]\n",
        "\t\tith_cluster_silhouette_values.sort()\n",
        "\t\tsize_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "\t\ty_upper = y_lower + size_cluster_i\n",
        "\t\tcolor = cm.nipy_spectral(float(i) / n_clusters)\n",
        "\t\tax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "\t\t\t\t\t\t\t0, ith_cluster_silhouette_values,\n",
        "\t\t\t\t\t\t\tfacecolor=color, edgecolor=color, alpha=0.7)\n",
        "\t\tax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\t\ty_lower = y_upper + 10\n",
        "\n",
        "\tax1.set_title(\"The silhouette plot for each cluster\")\n",
        "\tax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "\tax1.set_ylabel(\"Cluster label\")\n",
        "\tax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\tax1.set_yticks([])\n",
        "\tax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsrpxXqljJ-y"
      },
      "source": [
        "EvaluateClusterSilhouette(\n",
        "    X=df_transformed,\n",
        "    Clusters=X['Clusters'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlNCuj0JkIdd"
      },
      "source": [
        "# State and RainToday distribution per cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7MqATPNlsKk"
      },
      "source": [
        "variables_of_interest = ['State','RainToday', 'Cloud3pm']  # based on business acumen and PCA\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "for col in variables_of_interest:\n",
        "  df_stack = X.filter([col,'Clusters'],axis=1).groupby([col,'Clusters']).size().reset_index()\n",
        "  df_stack.rename({0:\"Count\"},axis=1,inplace=True)\n",
        "  fig = px.bar(df_stack, color = col, y = 'Count', x = 'Clusters', barmode = 'stack',width=None, height=400)\n",
        "  fig.update_xaxes(type='category',categoryorder='category ascending')\n",
        "  \n",
        "  print(f\"* Clusters per {col}\")\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "\n",
        "# 0 -  type of data that doenst happen in south of wales, victoria\n",
        "# 1 -  type of day that happens only in new south wales\n",
        "# 2 - type of day that happens every state: rain day\n",
        "# 3 - type of day that happens only in south of wales, victoria\n",
        "# 4 -  type of day that doesnt happens only in new south wales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "# Classifier to explain cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "* We need to find the most relevant variables, to define each cluster in terms of each relevant variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "source": [
        "df_clf = X.copy() #.sample(frac=0.051, random_state=config.RANDOM_STATE)\n",
        "df_clf['Clusters'] = df_clf['Clusters'].astype('int32')\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE,\n",
        "                                    # stratify=df_clf['Clusters']\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "* Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9nz9-UeBl0P"
      },
      "source": [
        "pipeline_clf_cluster['MedianImputer'].imputer_dict_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Cjsn6l5cqF"
      },
      "source": [
        "* Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QPwTqa0OcY"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train,y_train)\n",
        "\n",
        "# do GridCV after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        " * Evaluate model performance on Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\n",
        "      classification_report(y_train, pipeline_clf_cluster.predict(X_train))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "source": [
        "print(\n",
        "      classification_report(y_test, pipeline_clf_cluster.predict(X_test))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "* Check main features importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVMicAqrjflR"
      },
      "source": [
        "# after data cleaning and feat engine, the feature space changes\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "print(f\"* These are the {len(best_features)} most important features. \"\n",
        "      f\"The model was trained on them: \\n{best_features}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N7Mc3mrAYLq"
      },
      "source": [
        "df_feature_importance = pd.DataFrame(data={\n",
        "    'Attribute': best_features,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_\n",
        "  })\n",
        "\n",
        "df_feature_importance.sort_values(by='Importance', ascending=False).plot(kind='bar',x='Attribute',y='Importance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQblpQISv0Fd"
      },
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# _parameters = {\n",
        "#     'model__n_estimators':[50], # [100,200,50],\n",
        "#     'model__max_depth': [3] # [None,3,10]\n",
        "# }\n",
        "\n",
        "\n",
        "# _pipe = GridSearchCV(\n",
        "# \t\testimator = pipeline_clf_cluster,\n",
        "# \t\tparam_grid = _parameters, \n",
        "# \t\tcv=2,n_jobs=-2,verbose=2)\n",
        "# _pipe.fit(X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4baRUyFv0IL"
      },
      "source": [
        "# PipelineToDeploy = _pipe.best_estimator_\n",
        "# PipelineToDeploy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZ0bHjDyrMr"
      },
      "source": [
        "# _pipe.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwzpgGwys-x"
      },
      "source": [
        "# X_train.columns[PipelineToDeploy['feat_selection'].get_support()].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUEqXK0yu1F"
      },
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "# print( classification_report(y_test, PipelineToDeploy.predict(X_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUo1Odq3kRD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "# Clusters Profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "* Main variables that define a cluster\n",
        "\n",
        "1.   Using main features from previous classifier\n",
        "2.   And variables we are interested (busines acumen)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-k5S-NKFTMW"
      },
      "source": [
        "main_clusters_variables =  df_feature_importance['Attribute'].to_list() #+ ['RainToday'] # ['State','RainToday','Cloud3pm']\n",
        "main_clusters_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "source": [
        "df_cluster_profile = X.copy()\n",
        "for col in ['Cloud9am','Cloud3pm']:\n",
        "  df_cluster_profile[col] =df_cluster_profile[col].astype('object')\n",
        "\n",
        "\n",
        "df_cluster_profile = df_cluster_profile.filter(items=main_clusters_variables+['Clusters'],axis=1)\n",
        "\n",
        "num_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(include=['number']).columns.to_list()\n",
        "categorical_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(exclude=['number']).columns.to_list()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nl2EOOwtHdc"
      },
      "source": [
        "df_cluster_profile.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NBCTceJbgRu"
      },
      "source": [
        "## Custom Functions for Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a9x3gPbnwu"
      },
      "source": [
        "* Distribution profile for all clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIqEazT5blBg"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "def PlotClustersDistribution(df,num_var,categorical_var):\n",
        "  for col in num_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(10,5));\n",
        "    sns.kdeplot(data=df, x=col, hue=\"Clusters\",palette='Set2')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "  for col in categorical_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(15,5));\n",
        "    sns.countplot(data=df.sort_values(by=col), hue=col, x=\"Clusters\",palette='Set2')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEGYqOOJcKPK"
      },
      "source": [
        "* Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oxlK-aOcKX3"
      },
      "source": [
        "# def IndividualClusterAnalysis(df_cluster_profile):\n",
        "\n",
        "#   sns.set_style(\"darkgrid\")\n",
        "#   for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "#     df_cluster = df_cluster_profile.query(f\"Clusters == {cluster}\")\n",
        "#     print(f\"=================== Cluster {cluster} ===================\")\n",
        "    \n",
        "#     for col in num_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       plt.figure(figsize=(10,5));\n",
        "#       sns.histplot(data=df_cluster, x=col)\n",
        "#       plt.show();\n",
        "\n",
        "#       iqr = df_cluster[col].quantile([0.25,0.75])\n",
        "#       print(f\"* IQR: {iqr[0.25]} - {iqr[0.75]}\")\n",
        "#       print(\"\\n\")\n",
        "\n",
        "#     for col in categorical_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       try:\n",
        "#         plt.figure(figsize=(10,5));\n",
        "#         sns.countplot(data=df_cluster, x=col)\n",
        "#         freq = df_cluster[col].value_counts()\n",
        "#         plt.show()\n",
        "#       except Exception as e:\n",
        "#         print(e)\n",
        "#       print(\"\\n\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "* Description All Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}' ({CategoryPercentage}%) ; \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOOWOX14vQw"
      },
      "source": [
        "## All Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "DescriptionAllClusters(df_cluster_profile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxpVsX_t9Pwz"
      },
      "source": [
        "PlotClustersDistribution(df=df_cluster_profile, num_var=num_var, categorical_var=categorical_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdkP_KIA4ylt"
      },
      "source": [
        "## Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWpacqZncG29"
      },
      "source": [
        "# IndividualClusterAnalysis(df_cluster_profile)  # maybe remove? analysis above is better"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}