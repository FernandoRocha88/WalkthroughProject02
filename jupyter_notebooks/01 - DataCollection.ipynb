{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fetch data from Kaggle\n",
        "*   Fetch geospatial data\n",
        "*   Combine both and save.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "*  geospatial data **(I want this dataset to be pulled with SQL or to pull as an api)**\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Dataset outputs/datasets/collection/WeatherAustralia.csv\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "\n",
        "* Geospatial data is provided in a proper format for you\n",
        "  * As a raw format, it needed to be engineered separately. It didnt have all cities mapped to the dataset downloaded from Kaggle (WeatherAUS). This task was manual and is already done, so you dont have to worry about it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpTc5-PJVH_i"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhrJkTGtVMLQ"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SooBUDWVIQK"
      },
      "source": [
        "# this notebook doesnt need to install/update packages\n",
        "\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JfN05UMRtUd"
      },
      "source": [
        "* If you want to see which packages the session provides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRqt7wYNRm6k"
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* The notebook is setup already to use GPU, however, it is good to remind you the process\n",
        "\n",
        "* Go to Edit → Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBTpcRzwouQp"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStVvDjfTxAk"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8MWs250vtR"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "import uuid\n",
        "file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "print(\"\\n\\n\")\n",
        "os.remove(f\"{file_name}.txt\")\n",
        "! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (user email and password)\n",
        "os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS6BavwrkUeh"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwFQLlmwrl9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcmA1wG8AdC"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8eR_vMkbdZa"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGfSTuUAbdir"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dafOBor8OoM"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "! git add .\n",
        "! git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkyUs70oloW"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      },
      "source": [
        "!git push origin main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXBDTg2ouLC"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_twMc7cefGw"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LEJkEZwl2K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Fecth data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQrZFfd_J4Ix"
      },
      "source": [
        "* Make sure kaggle package is installed. In a Colab session, it normally should be. In case it is not, run the following command in a code cell: **! pip install -q kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "pip show kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DryO4viMfRc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPvhgJnFKOww"
      },
      "source": [
        "* You first need to download to your machine a **json file (authentication token)** from Kaggle for authentication. \n",
        "* The process is:\n",
        "  1. From the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings. Scroll down to the section of the page labelled API:\n",
        "  2. Click Expire API Token to remove previous tokens\n",
        "  3. To create a new token, click on the “Create New API Token” button. It will generate a fresh authentication token and will download kaggle.json file on your machine.\n",
        "  \n",
        "\n",
        "* In case you find any difficulty, go to \"Authentication\" section in this [link](https://www.kaggle.com/docs/api).\n",
        "\n",
        "\n",
        "\n",
        "* In the end, you should have this file saved locally in your machine. **Please make sure this file is labelled as kaggle.json**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9BaDpLjM-rP"
      },
      "source": [
        "* Upload to this Colab session your kaggle.json file\n",
        "* Once you run the cell below, Click on \"Choose Files\", find your kaggle.json file and select it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVlh18YgbyEg"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySAC0yIkNJm4"
      },
      "source": [
        "* Get the dataset path from the Kaggle url. When you are viewing the dataset at Kaggle, check what is after https://www.kaggle.com/ . You should copy that at KaggleDatasetPath.\n",
        "* Set your destination folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3u49Wn7byHA"
      },
      "source": [
        "KaggleDatasetPath = \"jsphyg/weather-dataset-rattle-package\"\n",
        "DestinationFolder = \"inputs/datasets/raw\"   \n",
        "!kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onuX0tVRNkB8"
      },
      "source": [
        "* Unzip the downloaded file, delete the zip file and delete kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG9kmnUcNVUh"
      },
      "source": [
        "!unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
        "  && rm {DestinationFolder}/*.zip \\\n",
        "  && rm kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHymXXJONnag"
      },
      "source": [
        "* Well done! You can now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)\n",
        "* The codes for executing that are in the section **\"Connection between: Colab Session and your GitHub Repo\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsezQ_UoPN6k"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/WalkthroughProject/inputs/datasets/raw/weatherAUS.csv\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkbvvaiRk3e8"
      },
      "source": [
        "* Renaming 'Rainfall' to 'RainfallToday'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRqGWi0Uk27X"
      },
      "source": [
        "df.rename(mapper={'Rainfall':'RainfallToday'},axis=1,inplace=True)\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3_f7npJyTz"
      },
      "source": [
        "* Set Date as datetime format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqIxFmWP651T"
      },
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3k6YZH0IiwT"
      },
      "source": [
        "* Are all dates in the proper sequence (with no missing DATES) for a given citiy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQqgdt5n70Cu"
      },
      "source": [
        "given_city = 'Albury'\n",
        "df_albury = df.query(f\"Location == '{given_city}'\").copy()\n",
        "\n",
        "df_no_missing_date = pd.DataFrame(\n",
        "    data={\"Date\":pd.date_range(start = df_albury['Date'].min(), end = df_albury['Date'].max() )})\n",
        "\n",
        "print(f\"* df_albury shape: {df_albury.shape} \\n\"\n",
        "      f\"* df_no_missing_date shape:{df_no_missing_date.shape} \\n\"\n",
        "      f\"* It means there are {len(df_no_missing_date) - len(df_albury)} days as missing dates for {given_city}. \\n\"\n",
        "      f\"* We should add these missing dates, before adding adding 'RainfallTomorrow'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxSnadf8SRg"
      },
      "source": [
        "* Does it happend only to Albury, or with other cities?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsfnh5wU8nUL"
      },
      "source": [
        "df_analysis = pd.DataFrame([])\n",
        "\n",
        "for city in df['Location'].unique():  \n",
        "  dfCity = df.query(f\"Location == '{city}'\").copy()\n",
        "  \n",
        "  dfAux = pd.DataFrame(\n",
        "      data={\"RowsFullRange\":len(pd.date_range(start = dfCity['Date'].min(), end = dfCity['Date'].max())),\n",
        "            \"RowsOriginal\": len(dfCity)\n",
        "      },\n",
        "      index=[city])\n",
        "  dfAux['Difference'] = dfAux[\"RowsFullRange\"] - dfAux[\"RowsOriginal\"]\n",
        "  df_analysis =df_analysis.append(dfAux)\n",
        "  \n",
        "df_analysis[['Difference']].hist(bins=50,figsize=(10,4));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKye2Kpn-1hk"
      },
      "source": [
        "* Only 3 cities dont have missing DATES!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQEHsSNf-LXG"
      },
      "source": [
        "df_analysis.sort_values(by='Difference').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xst6xCZh7o0L"
      },
      "source": [
        "* Add 'RainfallTomorrow' and 'RainYesterday'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meY-kMEiKHt-"
      },
      "source": [
        "def AddRainfallTomorrowAndRainYesterday(df):\n",
        "  df_final = pd.DataFrame([])\n",
        "  \n",
        "  for city in df['Location'].unique():  # loops on all cities\n",
        "\n",
        "    # subset data from given city\n",
        "    dfCity = df.query(f\"Location == '{city}'\").copy()\n",
        "    \n",
        "    # create dataframe with no missing date. It will have one column only\n",
        "    df_city_all_dates = pd.DataFrame(\n",
        "        data={\"Date\":pd.date_range(start = dfCity['Date'].min(), end = dfCity['Date'].max() )})\n",
        "    \n",
        "\n",
        "    # combine both (it will create many missing values, but there will be no missing dates)\n",
        "    df_city_all_dates = df_city_all_dates.merge(right=dfCity, how='left', on='Date', sort=True)\n",
        "\n",
        "\n",
        "    # Create RainfallTomorrow level, and RainYesterday\n",
        "    df_city_all_dates['RainfallTomorrow'] = df_city_all_dates['RainfallToday'].shift(-1)\n",
        "    # df_city_all_dates['RainYesterday'] = df_city_all_dates['RainToday'].shift(1)\n",
        "\n",
        "\n",
        "    # remove days where there is no data collecion from a given city\n",
        "    df_city_all_dates.dropna(subset=['Location'],inplace=True)\n",
        "\n",
        "    # append to final df\n",
        "    df_final = df_final.append(df_city_all_dates)\n",
        "  \n",
        "  df_final.reset_index(drop=True, inplace=True)\n",
        "  return df_final\n",
        "\n",
        "df = AddRainfallTomorrowAndRainYesterday(df)\n",
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOa2U7pv3oH-"
      },
      "source": [
        "# Load spatial data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0xq3RKL3se9"
      },
      "source": [
        "* The raw data, with a map for australian cities vs gps coordinates and state, was downloaded from: https://simplemaps.com/data/au-cities\n",
        "* However this dataset didnt have all locations present at weatherAUS.csv (like Uluru, PerthAirport, MelbourneAirport etc). We kindly added this information for you, so you dont have to worry about it for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaeUBBN348W-"
      },
      "source": [
        "spatial_data_link = (f\"https://raw.githubusercontent.com/{os.environ['UserName']}/{os.environ['RepoName']}/\"\n",
        "                     f\"main/inputs/datasets/raw/GeospatialAustralia.csv\")\n",
        "\n",
        "df_spatial = pd.read_csv(spatial_data_link)\n",
        "df_spatial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwJF1V74IcFT"
      },
      "source": [
        "* Let's filter the most relevant variables for this project and rename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCaxsjmKIbOE"
      },
      "source": [
        "df_spatial = (df_spatial\n",
        "              .filter(['city', 'lat', 'lng', 'admin_name'])\n",
        "              .rename(mapper={\"city\":\"Location\",\n",
        "                              \"lat\":\"Latitude\",\n",
        "                              \"lng\":\"Longitude\",\n",
        "                              \"admin_name\":\"State\"},axis=1)\n",
        "              )\n",
        "\n",
        "df_spatial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCIZ_bMMkJNF"
      },
      "source": [
        "* Does spatial dataset cover all cities from WeatherAUS dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVWzc6IeAmfW"
      },
      "source": [
        "count = 0\n",
        "list_of_cities = []\n",
        "for city_df in df.sort_values(by='Location')['Location'].unique():\n",
        "  if city_df not in df_spatial.sort_values(by='Location')['Location'].unique():\n",
        "    count +=1\n",
        "    print(f\"{city_df}\")\n",
        "\n",
        "print(f\"\\n\\n* There are {count} cities that are not mapped \\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbKon5GdlB0T"
      },
      "source": [
        "# Combining both datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjrZajC45Eer"
      },
      "source": [
        "df_combination = df.merge(right=df_spatial, how='left',on='Location')\n",
        "df_combination.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEjs4kDqJFB8"
      },
      "source": [
        "* Evaluating datasets shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qaFvT2F6ySq"
      },
      "source": [
        "print(f\"* df_combination.shape {df_combination.shape} \\n\"\n",
        "      f\"* df.shape {df.shape} \\n\"\n",
        "      f\"* df_spatial.shape {df_spatial.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXk-Xp6v9cy"
      },
      "source": [
        "* Check if there is missing data in the merged columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7eLI6D0vsnL"
      },
      "source": [
        "df_combination.filter(['Location','Latitude', 'Longitude','State']).isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jokafeqTllZb"
      },
      "source": [
        "* Check how many unieque cities are in the kaggle dataset (df) and geo spatial dataset (df_spatial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSNm0bMQjf7X"
      },
      "source": [
        "print(f\"* There are {df['Location'].nunique()} unique cities at df dataset. \\n\"\n",
        "      f\"* There are {df_spatial['Location'].nunique()} unique cities at df_spatial dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Saving final dataset and pushing to Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKlnIozA4eQO"
      },
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df_combination.to_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNVNpuV6NV2"
      },
      "source": [
        "* Well done! You can now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)\n",
        "  * The codes for executing that are in the section \"Connection between: Colab Session and your GitHub Repo\"\n",
        "* Then, save this notebook at your GitHub repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9BfhBCX6Nz5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}