{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Cleaning Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Evaluate missing data\n",
        "*   Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        " \n",
        "* Missing Data Imputers\n",
        "  * Drop Variables:  `['Evaporation', 'Cloud9am', 'Sunshine']`\n",
        "  * Drop Rows: `['RainTomorrow', 'RainYesterday', 'RainToday', 'RainfallTomorrow']`\n",
        "  * CategoricalImputer with \"Missing\": `['WindDir9am', 'WindGustDir', 'WindDir3pm', 'Cloud3pm']`\n",
        "  * Median Imputation: `['Pressure3pm', 'Pressure9am',WindGustSpeed','Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am','WindSpeed9am','Temp9am','MaxTemp','RainfallToday']`\n",
        "  * Mean Imputation: `['MinTemp']`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kspgyffxear-"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGT0ZCtwFAFv"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcidnQspZztu"
      },
      "source": [
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install missingno==0.4.2\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install ppscore==1.2.0\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit → Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStVvDjfTxAk"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8MWs250vtR"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "import uuid\n",
        "file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "print(\"\\n\\n\")\n",
        "os.remove(f\"{file_name}.txt\")\n",
        "! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwFQLlmwrl9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcmA1wG8AdC"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dafOBor8OoM"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkyUs70oloW"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      },
      "source": [
        "!git push origin main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXBDTg2ouLC"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_twMc7cefGw"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LEJkEZwl2K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"/content/WalkthroughProject02/outputs/datasets/collection/TelcoCustomerChurn.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Quick EDA with Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df=df, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvwabO0JsmYW"
      },
      "source": [
        "# Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qufL2HWrF8ig"
      },
      "source": [
        "* supporting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Zy_MglsmYo"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr(df,threshold):\n",
        "  if len(df.columns) > 1:\n",
        "    mask = np.zeros_like(df, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[abs(df) < threshold] = True\n",
        "\n",
        "    fig, axes = plt.subplots()\n",
        "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                     mask=mask, cmap='viridis', annot_kws={\"size\": 8}, ax=axes\n",
        "                     )\n",
        "    axes.set_yticklabels(df.columns, rotation = 0)\n",
        "    plt.ylim(len(df.columns),0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def heatmap_corr_pps(df,threshold):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,12))\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                        mask=mask,cmap='rocket_r', annot_kws={\"size\": 8})\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "  # heatmap_corr_pps(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "  # heatmap_corr_pps(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "  heatmap_corr_pps(df=pps_matrix,threshold=PPS_Threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryNo1VnXSK9K"
      },
      "source": [
        "* Calculate Correlations and Power Predictive Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Z4SXf6GbED"
      },
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ-0L4PiSPEK"
      },
      "source": [
        "* Display at Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioE3yuC4Q7QK"
      },
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,\n",
        "                  CorrThreshold=0.0, PPS_Threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYdZHyDhu4kG"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoVRefhu2Bk"
      },
      "source": [
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcocWZkIx6nk"
      },
      "source": [
        "* Custom function to display missing data levels in a dataframe, it shows the aboslute levels, relative levels and data type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9CoLqBhO7ga"
      },
      "source": [
        "def EvaluateMissingData(df):\n",
        "  missing_data_absolute = df.isnull().sum()\n",
        "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "  df_missing_data = (pd.DataFrame(\n",
        "                          data= {\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                 \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                 \"DataType\":df.dtypes}\n",
        "                                  )\n",
        "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
        "                    .query(\"PercentageOfDataset > 0\")\n",
        "                    )\n",
        "\n",
        "  return df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrzaG-ZhEKt"
      },
      "source": [
        "* Check missing data levels for initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxvNnLAjxCSi"
      },
      "source": [
        "EvaluateMissingData(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsFb6_NYyFgB"
      },
      "source": [
        "* Missing data levels in a visual format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1U17dNovahA"
      },
      "source": [
        "import missingno as mi\n",
        "import matplotlib.pyplot as plt\n",
        "mi.matrix(df=df, figsize=(20,6))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp-lSdVH-_-3"
      },
      "source": [
        "* Go to your spreadsheet at your Google Drive to list** potential approaches to handle missing data**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsL8yYQEyOSt"
      },
      "source": [
        "## Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_52ePITy6pQ"
      },
      "source": [
        "\n",
        "* It is assumed that you  already:\n",
        "  * assessed the missing data levels, \n",
        "  * did a quick EDA, \n",
        "  * checked correlation (pearson, spearman),\n",
        "  * checked power predictive score.\n",
        "* So you are aware of the variables to work on\n",
        "\n",
        "---\n",
        "\n",
        "* **Strategy**\n",
        "* First, for all variables you need to imput missing data, write potential imputation approach for data cleaning.\n",
        "  * Over the course, you saw multiple approaches for dealing with missing data, like DropVariables, DropNA, Imput with mean/median/mode, Imput the most frequent item etc\n",
        "\n",
        "* Then, you will **iterate the steps below across different imputation approaches**, so at the end you will have dealt with all variables with missing data\n",
        "\n",
        "  * 1 -  Select a **imputation approach**\n",
        "  * 2 - Select **variables** to apply the approach\n",
        "  * 3 - Create a **separate dataframe** applying this imputation approach to the selected variables\n",
        "  * 4 - **Compare** this new dataset with initial dataset to validate/assess the effect on distribution on variables\n",
        "  * 5 - **If** you are satisfied, **apply** the selected imputation approach to the initial dataframe\n",
        "  * 6 - **Evaluate** if you have more variables to deal. If yes, iterate. If not, you are done.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8Yqjy6ghyw"
      },
      "source": [
        "### Data Cleaning Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GaYe_DgqwT"
      },
      "source": [
        "* List here the imputation approaches you want initially to try.\n",
        "\n",
        "  * Drop Variables\n",
        "  * Drop Rows\n",
        "  * CategoricalImputer\n",
        "  * Median Imputation\n",
        "  * Mean Imputation\n",
        "\n",
        "* **The list above is your guide, your map to know in which stage you are in the data cleaning process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12Z8KIPoZ-8"
      },
      "source": [
        "### Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4T10lOcofen"
      },
      "source": [
        "* You have to split train and test set for cleaning the data\n",
        "  * Unless you consider only Drop Variables and Drop Rows, which is not the case.\n",
        "  * Hint: in the majority of the time in the workplace, you will need to split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knk7DcVborLI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from config import config\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['Churn'],\n",
        "                                        test_size=config.TEST_SIZE,\n",
        "                                        random_state=config.RANDOM_STATE)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdMPKBU_yCpF"
      },
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADd7Ruy2J8K"
      },
      "source": [
        "### DataCleaningEffect() function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrrjpCSCvLz8"
      },
      "source": [
        "* We create a custom function to evaluate variables distribution before and after applying the method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_N7yZVnPBqR"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # select variables in which the given data cleaning method was not applied  \n",
        "  variables_not_applied_with_method = [x for x in df_cleaned.columns if x not in variables_applied_with_method]\n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if variable is numerical, plots histogram, if categorical, plots barplot\n",
        "  for set_of_variables in [variables_applied_with_method,variables_not_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:   \n",
        "        # it is categorical variable: barplot\n",
        "\n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"]).set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: \n",
        "        # it is numerical variable: histogram\n",
        "\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\")\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\").set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uATij1hg9-CH"
      },
      "source": [
        "### Template For Data Cleaning (Replace with method name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU1WfV1t9-CW"
      },
      "source": [
        "* Step 1: Imputation approach: **write here the imputation approach name**\n",
        "* Step 2: Select variables to apply the approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUk1uIWA9-CX"
      },
      "source": [
        "##### list here the variables you want to apply the imputation approach\n",
        "variables_method = []\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply. \\n\\n{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkEkjOSO9-CX"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9Vq5BI9-CY"
      },
      "source": [
        "##### create a df_method dataframe applying your imputation approach to the TrainSet\n",
        "df_method = ...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufMCbCC9-CY"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Oamd2q9-CY"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJsB1uh9-CZ"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSnOlmfd9-CZ"
      },
      "source": [
        "### Apply your method to the Train and Test Set\n",
        "TrainSet, TestSet = .....\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6iVZgMu9-CZ"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9id5pkns9-Ca"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUeGUUmu4qru"
      },
      "source": [
        "### Drop Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTFTio14qr8"
      },
      "source": [
        "* Hint: you may drop Variables with more than 80% of missing data, since these variables will likely not add much value. However, this is not the case in this dataset\n",
        "* Step 1: imputation approach: **Drop Variables**\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlQBeLo44qr8"
      },
      "source": [
        "variables_method = ['Evaporation', 'Cloud9am', 'Sunshine', \"MinTemp\",\"Temp9am\", \"Temp3pm\",\n",
        "                    \"WindGustSpeed\", \"Humidity9am\", \"Pressure9am\"]\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
        "    f\"{variables_method}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7au_UdA34qr9"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh9E2cEe4qr9"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsjRXMlU4qr9"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9pg4ww_BO-"
      },
      "source": [
        "* In this case, no effect on variables distribution, since you are not removing rows, but columns\n",
        "* The effect might be losing features that might have a relevant impact in your machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBq1a_Me4qr-"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGO5M1k44qr-"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LozxNCVO4qr_"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGmZy46L4qr_"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9E7EKFiAsuw"
      },
      "source": [
        "### Drop Rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSaadrA2Asu4"
      },
      "source": [
        "* Step 1: imputation approach: **Drop Rows / Observations**\n",
        "* Hint: as a rule of thumb, you can drop the rows if a variable has less than 5% of missing data. However, you can factor the effect of removing it. Eventually for your dataset, 5% is a relevant amount of data or eventually these rows are significant for other variables.\n",
        "* You should also consider the variable distribution for that particular variables to decide whether drop or not these rows.\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOujkgg3Asu5"
      },
      "source": [
        "variables_method = ['RainTomorrow', 'RainToday', 'RainfallTomorrow']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this imputation approach. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDiuzFVAsu7"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXE-HzytAsu7"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "imputer = DropMissingData(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)\n",
        "\n",
        "lost_percentage = round(100 - len(df_method)/len(TrainSet)*100 ,2)\n",
        "\n",
        "print(f\"* If I apply this imputer, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(TrainSet)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMqogFarAsu8"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrGdmz3Asu8"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5_9TwaAsu8"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUvHvBlAsu9"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "imputer = DropMissingData(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6fVg28Asu9"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOZ6eWTuAsu-"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPmsfd5enMJS"
      },
      "source": [
        "### CategoricalImputer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZhlXhanMJY"
      },
      "source": [
        "* Step 1: imputation approach: **The CategoricalImputer() replaces missing data in categorical variables with the string ‘Missing’**\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbSBiHTBnMJY"
      },
      "source": [
        "variables_method = ['WindDir9am', 'WindGustDir', 'WindDir3pm', 'Cloud3pm']\n",
        "print(f\"* {len(variables_method)} variables to apply \\n\\n{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3iiDrOnMJZ"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjowIPUnMJZ"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "imputer = CategoricalImputer(variables=variables_method,imputation_method='missing',fill_value='Missing')\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9R_Em4JnMJa"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFKQWlk0nMJa"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1YjY2CnMJa"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdpDbaKnMJb"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "imputer = CategoricalImputer(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4PiB9wMnMJb"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3feT1lbnMJb"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9lrYiU4FH1"
      },
      "source": [
        "### Median Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uARU9Bjf4FIA"
      },
      "source": [
        "* Step 1: imputation approach: **mediam imputation**\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPUN4yLa4FIC"
      },
      "source": [
        "variables_method = ['Pressure3pm',\n",
        "                    'Humidity3pm', 'WindSpeed3pm',\n",
        "                    'WindSpeed9am','MaxTemp','RainfallToday']\n",
        "                    \n",
        "\n",
        "variables_method = ['TotalCharges']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPuZGRkC4FID"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZJKY5Aj4FID"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYFuuEqh4FIE"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o3k-nMU4FIE"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quYOLkKX4FIF"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2OEFGEa4FIF"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrD9q87M4FIG"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d_DiCUG4FIG"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzlAb654LPY"
      },
      "source": [
        "### Mean Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0YSkpN4LPZ"
      },
      "source": [
        "* Step 1: imputation approach: **mean imputation**\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TffoUDqS4LPa"
      },
      "source": [
        "# variables_method = ['MinTemp']\n",
        "\n",
        "# print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "#     f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA21rw_04LPa"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeolVUEj4LPb"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_JVWwwl4LPb"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeXNWmG4LPc"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhoe836Z4LPc"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaCyiWO4LPd"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmYtf324LPe"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPD4bPIz4LPe"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjX8r9DN8eeh"
      },
      "source": [
        "EvaluateMissingData(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oexJ3aqG8rLS"
      },
      "source": [
        "* Well done! Your data is cleaned!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SJssRvpgS0A"
      },
      "source": [
        "print(TrainSet.shape)\n",
        "TrainSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD--GI5Jvp8h"
      },
      "source": [
        "# Save cleaned data: Train/Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNWONrYwu6d"
      },
      "source": [
        "TrainSet.to_csv(\"/content/WalkthroughProject02/outputs/datasets/cleaned/TrainSetCleaned_Telco.csv\",index=False)\n",
        "TestSet.to_csv(\"/content/WalkthroughProject02/outputs/datasets/cleaned/TestSetCleaned_Telco.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMucnZHuD3CP"
      },
      "source": [
        "* You may now go to \"Push generated/new files from this session to GitHub Repo\" section and push these files to the repo"
      ]
    }
  ]
}