{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Classification Sklearn.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a classification model to predict if a prospect will churn or not.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Data cleaning and Feature Engineering pipeline; Modeling pipeline; X_train columns\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.24.2\n",
        "! pip install imbalanced-learn==0.8.0\n",
        "# ! pip install pandas-profiling==2.11.0\n",
        "# ! pip install ppscore==1.2.0\n",
        "# ! pip install pingouin==0.3.12\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "import uuid\n",
        "file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "print(\"\\n\\n\")\n",
        "os.remove(f\"{file_name}.txt\")\n",
        "! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "      .drop(labels=['tenure','customerID','TotalCharges'],axis=1)  \n",
        "                    # target variable for regressor, remove from classifier  \n",
        "                    # drop other variables we will not need for this project\n",
        "  )\n",
        "\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ4IW652pgZL"
      },
      "source": [
        "* We know already in ufront that **Train Set Target (Rain Tomorrow) is imbalanced**\n",
        "  * We will apply SMOTE technique to handle that. That was covered in Develop & Deploy an AI System - Target Imbalance\n",
        "  * Therefore, we will produce 2 ML Pipelines:\n",
        "    * One for Data Cleaning and Feature Engineering\n",
        "    * Another for Feature Scaling, Feature Selection and Modeling\n",
        "  * The pipelines will be used to train the pipeline, to test the pipeline and to predict on live data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# ML Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCsXhBYVBJw"
      },
      "source": [
        "## Load estimators needed at Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk1RSVSYVBWw"
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.svm import SVC \n",
        "from sklearn.svm import LinearSVC \n",
        "from sklearn.svm import NuSVC \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.naive_bayes import ComplementNB \n",
        "from sklearn.naive_bayes import BernoulliNB \n",
        "from sklearn.naive_bayes import CategoricalNB \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## Data Cleaninig And Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary', \n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod']\n",
        "                                                  )\n",
        "      ),\n",
        "       \n",
        "\n",
        "       \n",
        "      # (\"DropFeatures\",DropFeatures(features_to_drop = ['OnlineSecurity', 'DeviceProtection', 'TechSupport'])\n",
        "      # ),\n",
        "       \n",
        "      (\"SmartCorrelatedSelection\",SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                           threshold=0.6,selection_method=\"variance\")\n",
        "      ),\n",
        "       \n",
        "       \n",
        "       \n",
        "      \n",
        "       \n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7BXNYMULrf"
      },
      "source": [
        "## Hyperparameter Optmization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Pipeline Optmization: Feature Scaling, Feature Selection and Model\n",
        "def PipelineClfSMOTE(model):\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "       (\"scaler\",StandardScaler() ),\n",
        "       (\"feat_selection\",SelectFromModel(model) ),\n",
        "       (\"model\",model ),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base\n",
        "\n",
        "# Custom Class for hyperparameter Optmization\n",
        "class HyperparameterOptmizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model=  PipelineClfSMOTE(self.models[key])\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs    \n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            # print(k)\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-WuiIvawEm"
      },
      "source": [
        "## Data before hitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWYdGoVMdazt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr_pps(df,threshold):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,12))\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                        mask=mask,cmap='rocket_r', annot_kws={\"size\": 8})\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr_pps(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr_pps(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "  heatmap_corr_pps(df=pps_matrix,threshold=PPS_Threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMwlRwlhayKq"
      },
      "source": [
        "FeaturesTrainSet = df.copy().drop(['Churn'],axis=1)\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(FeaturesTrainSet)\n",
        "                                        .columns)\n",
        "\n",
        "\n",
        "pipeline_before_model = PipelineDataCleaningAndFeatureEngineering()\n",
        "df_before_hitting_model = pd.DataFrame(data = pipeline_before_model.fit_transform(FeaturesTrainSet),\n",
        "                                       columns = columns_after_data_cleaning_feat_eng\n",
        "                                       )\n",
        "\n",
        "df_before_hitting_model = pd.concat([df_before_hitting_model,df['Churn']],axis=1)\n",
        "\n",
        "print(df_before_hitting_model.shape)\n",
        "df_before_hitting_model.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eRWKKO9lx9F"
      },
      "source": [
        "Why?????"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQj9jVkfkn56"
      },
      "source": [
        "pipeline_before_model = PipelineDataCleaningAndFeatureEngineering()\n",
        "pipeline_before_model.fit_transform(FeaturesTrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj_t4E8LkwH8"
      },
      "source": [
        "pipeline_before_model['SmartCorrelatedSelection'].features_to_drop_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6MYVZrYcwph"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df=df_before_hitting_model, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sT5XfcVdk1V"
      },
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_before_hitting_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05X9aVIkdlUj"
      },
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold=0.6, PPS_Threshold=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUmGuf3TUkfE"
      },
      "source": [
        "## Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-mLYa7Usch"
      },
      "source": [
        "### Quick Search using model's default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXpLRzZUkoH"
      },
      "source": [
        "models_quick_search = {\n",
        "    # 'RidgeClassifier':RidgeClassifier(config.RANDOM_STATE),\n",
        "    # \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"RandomForestClassifier\":RandomForestClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"AdaBoostClassifier\":AdaBoostClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=config.RANDOM_STATE),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    'RidgeClassifier': {},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"DecisionTreeClassifier\":{},\n",
        "    \"RandomForestClassifier\":{},\n",
        "    \"GradientBoostingClassifier\":{},\n",
        "    \"ExtraTreesClassifier\":{},\n",
        "    \"AdaBoostClassifier\":{},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"LogisticRegression\":{},\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzyKHnhVVlMu"
      },
      "source": [
        "from sklearn.metrics import f1_score, make_scorer\n",
        "quick_search = HyperparameterOptmizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "quick_search.fit(X_train,y_train, scoring='roc_auc', n_jobs=-1, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fL3YG8LVvOs"
      },
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='max_score')\n",
        "grid_search_summary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHovHEV6Vwsg"
      },
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu-Iu4p3VyFy"
      },
      "source": [
        "grid_search_pipelines[best_model].best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZtkABXdVzPJ"
      },
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVO0_T40V3lm"
      },
      "source": [
        "# after data cleaning, the feature space changes\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf['feat_selection'].get_support()].to_list()\n",
        "print(f\"* These are the {len(best_features)} most important features. \"\n",
        "      f\"The model was trained on them: \\n{best_features}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSkMvBgMErJ1"
      },
      "source": [
        "# SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUcOp83jy0QG"
      },
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQTZ_fRGyQFB"
      },
      "source": [
        "* Quick recap in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKjia1_VyQrH"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY3JCy2hq2OH"
      },
      "source": [
        "* Split Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vqzNI2zF1sZ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['Churn'],axis=1),\n",
        "                                    df['Churn'],\n",
        "                                    test_size = config.TEST_SIZE,\n",
        "                                    random_state = config.RANDOM_STATE,\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zBysp0tyqR2"
      },
      "source": [
        "## SMOTE: Target Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfzOprjgq4bO"
      },
      "source": [
        "* Fit DataCleaning And FeatureEngineering Pipeline\n",
        "  * It is used to process train data, so SMOTE can be applied before training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsQRvnn1GI_d"
      },
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuq3902arZAz"
      },
      "source": [
        "* Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I28ACrp-rPgF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgoR6lTrKqY"
      },
      "source": [
        "* Use SMOTE to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP1JIwXNEsXO"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=config.RANDOM_STATE)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJO6V5zrdnw"
      },
      "source": [
        "* Check Train Set Target distribution after SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQdvEvNRG80Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xTTXMayvo6"
      },
      "source": [
        "## Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cwUldIJrjni"
      },
      "source": [
        "* Define models and parameters, for Quick Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMgswohfKBda"
      },
      "source": [
        "models_quick_search = {\n",
        "    'RidgeClassifier':RidgeClassifier(config.RANDOM_STATE),\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"RandomForestClassifier\":RandomForestClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"AdaBoostClassifier\":AdaBoostClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=config.RANDOM_STATE),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    'RidgeClassifier': {},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"DecisionTreeClassifier\":{},\n",
        "    \"RandomForestClassifier\":{},\n",
        "    \"GradientBoostingClassifier\":{},\n",
        "    \"ExtraTreesClassifier\":{},\n",
        "    \"AdaBoostClassifier\":{},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"LogisticRegression\":{},\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXu0Ryeown7N"
      },
      "source": [
        "* Quick GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7eLJcKEKBlQ"
      },
      "source": [
        "from sklearn.metrics import f1_score, make_scorer\n",
        "quick_search = HyperparameterOptmizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "quick_search.fit(X_train, y_train, scoring='f1', n_jobs=-1, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0bkL-IxwnJx"
      },
      "source": [
        "* Check results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpFOc7OAKMuz"
      },
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfoII4RjwkF2"
      },
      "source": [
        "* Check the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvYM0pfNMHv3"
      },
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htAXEVFpwiBV"
      },
      "source": [
        "* Parameters for best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDIt27RdKOG8"
      },
      "source": [
        "grid_search_pipelines[best_model].best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnJQlDlw1FE"
      },
      "source": [
        "* Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLotNfy4MKDE"
      },
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n30pl2dowzW3"
      },
      "source": [
        "* Most important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XGczhv2uo2C"
      },
      "source": [
        "best_features = X_train.columns[pipeline_clf['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Attribute': X_train.columns[pipeline_clf['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Attribute'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Attribute',y='Importance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Classifier Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gkzUT-xW3jx"
      },
      "source": [
        "## Custom Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myG6tDSGan4r"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "def PredictionEvaluation(X,y,pipeline,LabelsMap):\n",
        "\n",
        "  prediction = pipeline.predict(X)\n",
        "\n",
        "  Map = list() \n",
        "  for key, value in LabelsMap.items():\n",
        "    Map.append( str(key) + \": \" + value)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "        columns=[ [\"Actual \" + sub for sub in Map] ], \n",
        "        index= [ [\"Prediction \" + sub for sub in Map ]]\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction),\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PerformanceTrainTestSet(X_train,y_train,X_test,y_test,pipeline,LabelsMap):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  PredictionEvaluation(X_train,y_train,pipeline,LabelsMap)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  PredictionEvaluation(X_test,y_test,pipeline,LabelsMap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpUfEAGlW5aK"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umWjIvGMNLig"
      },
      "source": [
        "PerformanceTrainTestSet(X_train=X_train, y_train=y_train,\n",
        "                        X_test=X_test, y_test=y_test,\n",
        "                        pipeline=pipeline_clf,\n",
        "                        LabelsMap= {0:\"No Churn\", 1:\"Yes Churn\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBVunRgBqIXQ"
      },
      "source": [
        "# Save Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16bIOgs3J7OD"
      },
      "source": [
        "import joblib\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbbAO2r248W"
      },
      "source": [
        "* We will save 2 pipelines\n",
        "  * Both should be used in conjuntion to predict on Train Set, Test Set and Live Data\n",
        "* Pipeline responsible for Data Cleaning and Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCcAlvoG3CRm"
      },
      "source": [
        "pipeline_data_cleaning_feat_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaHdCf4HKBLg"
      },
      "source": [
        "try:\n",
        "  os.makedirs(name='outputs/ml_pipeline/predict_churn')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
        "            filename=\"outputs/ml_pipeline/predict_churn/clf_pipeline_data_cleaning_feat_eng.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-iU6TL3LVI"
      },
      "source": [
        "* Pipeline responsible for Feature Scaling, Feature Selection and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zEBxfvBqI29"
      },
      "source": [
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObL5Iz8tKdsZ"
      },
      "source": [
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=\"outputs/ml_pipeline/predict_churn/clf_pipeline_model.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZY7eJblSGbd"
      },
      "source": [
        "We will save the TrainSet column's names, so we can use it to create our User Interface at Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z2aB72_TpBm"
      },
      "source": [
        "X_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Ukn_PPSUQt"
      },
      "source": [
        "joblib.dump(value=X_train.columns ,\n",
        "            filename=\"outputs/ml_pipeline/predict_churn/X_train_columns.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}